{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from A import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the IMDB movie review dataset\n",
    "# train_data = imdb.read_imdb(\"Datasets/aclImdb\", \"train\")\n",
    "# test_data = imdb.read_imdb(\"Datasets/aclImdb\", \"test\")\n",
    "\n",
    "# # Clean the loaded data using the clean_all_texts function\n",
    "# clean_train_data = {\"text\": imdb.clean_all_texts(train_data), \"label\": train_data[\"label\"]}\n",
    "# clean_test_data = {\"text\": imdb.clean_all_texts(test_data), \"label\": test_data[\"label\"]}\n",
    "\n",
    "# # Initialize a BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize the cleaned data\n",
    "# train_token = tokenizer(clean_train_data[\"text\"], padding='max_length', truncation=True, max_length=512)\n",
    "# test_token = tokenizer(clean_test_data[\"text\"], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# # Save the cleaned data as JSON files\n",
    "# with open(\"A/Preprocessed_Data/clean_train_data.json\", \"w\") as f:\n",
    "#     json.dump(clean_train_data, f)\n",
    "# with open(\"A/Preprocessed_Data/clean_test_data.json\", \"w\") as f:\n",
    "#     json.dump(clean_test_data, f)\n",
    "\n",
    "# # Save the tokenized data as PyTorch tensors\n",
    "# torch.save(train_token, \"A/Preprocessed_Data/train_token.pt\")\n",
    "# torch.save(test_token, \"A/Preprocessed_Data/test_token.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and tokenized IMDB movie review data\n",
    "clean_train_data, clean_test_data, train_token, test_token = imdb.load_imdb_data()\n",
    "\n",
    "# Calculate the length of each review in the training and testing data\n",
    "train_lengths = [len(text.split()) for text in clean_train_data[\"text\"]]\n",
    "test_lengths = [len(text.split()) for text in clean_test_data[\"text\"]]\n",
    "\n",
    "# Plot the distribution of review lengths in the training and testing data\n",
    "imdb.plot_length_distribution(train_lengths, test_lengths)\n",
    "\n",
    "# Select a balanced subset of the testing data\n",
    "selected_test_data = imdb.select_test(clean_test_data)\n",
    "\n",
    "# Create PyTorch datasets from the tokenized data and labels\n",
    "train_dataset = imdb.IMDBDataset(train_token, clean_train_data[\"label\"])\n",
    "test_dataset = imdb.IMDBDataset(test_token, selected_test_data[\"label\"])\n",
    "\n",
    "# Create PyTorch data loaders from the datasets\n",
    "# The data loaders will provide batches of data to the model during training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "# Move the model to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "# Set the number of training epochs\n",
    "epochs = 3\n",
    "# Calculate the total number of training steps\n",
    "total_steps = len(train_loader) * epochs\n",
    "# Calculate the number of warmup steps\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "# Initialize the gradient scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize lists to store the loss and accuracy values for each epoch\n",
    "train_loss_list, train_accuracy_list = [], []\n",
    "val_loss_list, val_accuracy_list = [], []\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Train the model for one epoch and get the training loss and accuracy\n",
    "    train_loss, train_accuracy = imdb.train(model, train_loader, optimizer, scheduler, scaler)\n",
    "    # Evaluate the model on the validation data and get the validation loss and accuracy\n",
    "    val_loss, val_accuracy, predictions, true_labels = imdb.evaluate(model, test_loader)\n",
    "    # Append the loss and accuracy values to the respective lists\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "    # Print the loss and accuracy values for this epoch\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"A/fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"A/fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load the model and tokenizer for prediction\n",
    "model = BertForSequenceClassification.from_pretrained(\"A/fine_tuned_bert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"A/fine_tuned_bert\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_samples(clean_test_data, num_samples, tokenizer, model, device):\n",
    "    indices = random.sample(range(len(clean_test_data['text'])), num_samples)\n",
    "\n",
    "    for i in indices:\n",
    "        text = clean_test_data['text'][i]\n",
    "        label = clean_test_data['label'][i]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1)\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"True label: {'positive' if label == 1 else 'negative'}\")\n",
    "        print(f\"Predicted label: {'positive' if predicted_class[0] == 1 else 'negative'}\")\n",
    "        print(f\"Probabilities: {probabilities[0].cpu().numpy().tolist()}\")\n",
    "        print()\n",
    "\n",
    "predict_samples(clean_test_data, 5, tokenizer, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
